{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFTmO5i6EN+tWmv48oawfq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silassjjunior/An-lise-de-Amea-as-STRIDE-com-IA-Open-Source/blob/main/Cerebro_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcYc1f3NS4d4"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# PASSO 1: INSTALAR TODAS AS DEPEND√äNCIAS\n",
        "# -----------------------------------------------------------------------------\n",
        "# Usamos o -q para uma instala√ß√£o \"quieta\", sem poluir a sa√≠da.\n",
        "!pip install -q transformers torch bitsandbytes accelerate Pillow flask pyngrok flask-cors\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PASSO 2: IMPORTAR BIBLIOTECAS E CONFIGURAR O MODELO DE IA\n",
        "# -----------------------------------------------------------------------------\n",
        "import torch\n",
        "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
        "from PIL import Image\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok, conf\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "import threading\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas. Iniciando a configura√ß√£o do modelo...\")\n",
        "\n",
        "# Configura√ß√£o do Modelo\n",
        "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
        "\n",
        "# Carregando o modelo em 4-bit (quantizado) para economizar mem√≥ria da GPU.\n",
        "# Isso √© crucial para rodar no ambiente gratuito do Colab.\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"  # Mapeia o modelo para a GPU automaticamente\n",
        ")\n",
        "\n",
        "# Carregando o processador, que prepara a imagem e o texto para o modelo.\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "print(\"‚úÖ Modelo de IA e processador carregados com sucesso na GPU.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PASSO 3: DEFINIR A FUN√á√ÉO DE AN√ÅLISE (O C√âREBRO)\n",
        "# -----------------------------------------------------------------------------\n",
        "def analyze_architecture(image_bytes, system_description):\n",
        "    \"\"\"\n",
        "    Analisa uma imagem de arquitetura usando o modelo LLaVA.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Abre a imagem a partir dos bytes recebidos na requisi√ß√£o.\n",
        "        raw_image = Image.open(image_bytes).convert('RGB')\n",
        "\n",
        "        # --- ENGENHARIA DE PROMPT ---\n",
        "        # Este √© o cora√ß√£o da nossa l√≥gica. Instru√≠mos a IA sobre o que fazer.\n",
        "        # O formato USER: <image>\\n... ASSISTANT: √© espec√≠fico do LLaVA.\n",
        "        prompt = (\n",
        "            \"USER: <image>\\n\"\n",
        "            \"A imagem mostra a arquitetura de um sistema. A descri√ß√£o do sistema √©: '{}'.\\n\\n\"\n",
        "            \"Aja como um especialista em seguran√ßa de aplica√ß√µes. Analise a arquitetura e a descri√ß√£o \"\n",
        "            \"para identificar poss√≠veis amea√ßas de seguran√ßa com base na metodologia STRIDE (Spoofing, \"\n",
        "            \"Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege). \"\n",
        "            \"Retorne sua an√°lise em um formato JSON v√°lido. O JSON deve ter uma chave 'analysis' que cont√©m \"\n",
        "            \"uma lista de objetos. Cada objeto deve representar um componente da arquitetura e deve conter \"\n",
        "            \"as chaves 'component', 'threats' (uma lista de objetos de amea√ßa) e 'suggestions' (uma lista de strings). \"\n",
        "            \"Cada objeto de amea√ßa deve ter as chaves 'stride_category' e 'threat_description'.\"\n",
        "            \"\\nASSISTANT:\"\n",
        "        ).format(system_description)\n",
        "\n",
        "        # Prepara as entradas (imagem e texto) para o modelo.\n",
        "        inputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(\"cuda\", torch.float16)\n",
        "\n",
        "        # Gera a resposta da IA.\n",
        "        output = model.generate(**inputs, max_new_tokens=2048)\n",
        "\n",
        "        # Decodifica a sa√≠da e extrai apenas a parte gerada pelo assistente (o JSON).\n",
        "        response_text = processor.decode(output[0], skip_special_tokens=True)\n",
        "        json_part = response_text.split(\"ASSISTANT:\")[1].strip()\n",
        "\n",
        "        print(\"‚úÖ An√°lise gerada pela IA.\")\n",
        "        return json_part\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro durante a an√°lise da IA: {e}\")\n",
        "        return None\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# PASSO 4: CRIAR A API COM FLASK E EXP√î-LA COM NGROK\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"üöÄ Iniciando o servidor Flask e o t√∫nel Ngrok...\")\n",
        "\n",
        "# adicione seu token de autentica√ß√£o do Ngrok na linha abaixo\n",
        "# Voc√™ pode obt√™-lo gratuitamente em https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "conf.get_default( ).auth_token = \"COLE O SEU TOKEN AQUI\" # cole o seu token\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Habilita CORS para permitir requisi√ß√µes do frontend local.\n",
        "\n",
        "@app.route('/analyze', methods=['POST'])\n",
        "def handle_analysis():\n",
        "    if 'image' not in request.files:\n",
        "        return jsonify({\"error\": \"Nenhuma imagem enviada\"}), 400\n",
        "\n",
        "    image_file = request.files['image']\n",
        "    description = request.form.get('description', 'Nenhuma descri√ß√£o fornecida.')\n",
        "\n",
        "    # Passamos os bytes da imagem diretamente para a fun√ß√£o de an√°lise.\n",
        "    analysis_json_str = analyze_architecture(image_file.stream, description)\n",
        "\n",
        "    if analysis_json_str:\n",
        "        try:\n",
        "            # Validamos se a string recebida √© um JSON v√°lido antes de retornar.\n",
        "            analysis_data = jsonify(eval(analysis_json_str.replace(\"```json\", \"\").replace(\"```\", \"\")))\n",
        "            print(\"‚úÖ Resposta JSON enviada com sucesso.\")\n",
        "            return analysis_data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Erro ao converter a resposta da IA para JSON: {e}\")\n",
        "            return jsonify({\"error\": \"A IA n√£o retornou um JSON v√°lido.\", \"raw_response\": analysis_json_str}), 500\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Falha ao gerar a an√°lise.\"}), 500\n",
        "\n",
        "# Fun√ß√£o para rodar o app Flask em uma thread separada\n",
        "def run_app():\n",
        "    app.run(port=5000, host='0.0.0.0')\n",
        "\n",
        "# Inicia o t√∫nel Ngrok e o servidor Flask\n",
        "# O Ngrok criar√° uma URL p√∫blica para nossa API Flask.\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"‚úÖ C√©rebro de IA est√° no ar! URL p√∫blica: {public_url}\")\n",
        "\n",
        "# Inicia o servidor Flask em uma thread para n√£o bloquear a c√©lula do Colab\n",
        "threading.Thread(target=run_app).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WRGclWeosSm_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}